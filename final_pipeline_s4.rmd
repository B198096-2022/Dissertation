---
title: "Updated_Pipelies_s4"
author: "B198096"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r}
#DoubletFinder is currently incompatible with Seurat v5 objects 
#Soooo I need to set up an R session with the old Seurat and SeuratObject 
#Run DoubeltFinder in this session, save the DF results
#Then return to the rest of the pipeline as add these results to the s5 objects 

#These two packages are used to download old package versions 
library("remotes")
library("fs")

#.libPaths() tells you where your packages are stored 
#"/Library/Frameworks/R.framework/Versions/4.2/Resources/library"
normal_lib <- "/Library/Frameworks/R.framework/Versions/4.2/Resources/library"
#I created a new directory that branches from the standard package directory
old_lib <- "/Library/Frameworks/R.framework/Versions/4.2/Resources/old-versions"

#Then I used the below commands to install the old Seurat and SeuratObject 
#install_version("Seurat", version = "4.3", lib = old_lib)
#install_version("SeuratObject", version = "4.1.3", lib = old_lib)
library("SeuratObject", lib.loc = old_lib)
library("Seurat", lib.loc = old_lib)

#And finally, load in the other required packages
library(dplyr)
library(patchwork)
library(sctransform)
library(celda)
library(DoubletFinder)
library(ggplot2)
#BiocManager::install("glmGamPoi")
#not binary
library(glmGamPoi)
setwd("/Volumes/cmvm/scs/groups/pramacha-GROUP/Max_Hammer")
```


## Step 1 : Specify the data sets you want processed  

```{r, specify data}

#Specify the files you want to run 

#How many samples are there in your data set
#Example: zhang healthy has 20 samples so range <- 1:20
range <- 1:6
#Specify the author of the data set for labeling, example: "zhang" 
sample_author <- "guilliams"
#And the file label for the nomenclature scheme being used
#Examples: "zhang_healthy_" or "guilliams_healthy_cell_" if specifying sc/sn 
sample_name <- "guilliams_fibrosis_cell_"
#Add sample abbreviation for meta data annotation, Example: "zh" or "zhc" 
sample_label <- "gfc"

#Specify where the samples are stored (cellranger output directories)
#Example: datadir <- "/Volumes/cmvm/scs/groups/pramacha-GROUP/Max_Hammer/cellranger/"
#Here the "cellranger" datadir holds a directory for each author, 
#with subdirs for each sample. This is the default of the submit.jobs.sh scripts 
datadir <- "/Volumes/cmvm/scs/groups/pramacha-GROUP/Max_Hammer/cellranger/"
#Specify where to upload the outputs to 
outdir <- "~/Desktop/Diss_final_run/"
#outdir <- "/Volumes/cmvm/scs/groups/pramacha-GROUP/Max_Hammer/QC/decontx_after/"

#Specify desired filtering parameters 
mt_cutoff <- 30
nfeature_cutoff <- 100
ncount_cutoff <- 0

#This loop generates lists of file locations and destinations 
Files <- list()
Raws <- list()
Destination <- list()
for (i in range){
  Files[[i]] <- paste0(datadir,sample_author,"/",sample_name, i,"/outs/filtered_feature_bc_matrix/")
  Raws[[i]] <- paste0(datadir,sample_author,"/",sample_name, i, "/outs/raw_feature_bc_matrix/")
  Destination[[i]] <- paste0(outdir,sample_name,i,"_UMAP.rds")
}


#Files <- paste0(datadir,sample_author,"/",sample_name, range,"/outs/filtered_feature_bc_matrix/")
#Raws <- paste0(datadir,sample_author,"/",sample_name, range, "/outs/raw_feature_bc_matrix/")
#Destination <- paste0(outdir,sample_name,range,"_UMAP.rds")

```

## Step 2 : Speficy Meta Data  

```{r, specify meta data}
#Provide a list of the meta data in the order that the samples are loaded
#Patient IDs 
patient <- c("bfc_p1", "bfc_p2", "bfc_p2", "bfc_p3", "bfc_p4", "bfc_p4")

#Patient Sex
sex <- c("Female", "Female", "Female", "Female", "Female", "Female")

#Patient age, I am using 10 year bins 
age <- c("70-79", "70-79", "70-79", "70-79", "40-49", "40-49")

#Any information about disease/cause of resection 
aetiology <- c("Symptomatic cholecystolithiasis", "Hepatocellular carcinoma", "Hepatocellular carcinoma", "Morbid obesity", "Symptomatic cholecystolithiasis Liver", "Symptomatic cholecystolithiasis")

#Fibrosis score from 0-4
fibrosis_score <- c("unknown", "unknown", "unknown", "unknown")

#fibrosis stage, from none-low(0-1), moderate (2-3), severe(4+)
fibrosis_stage <- c("moderate", "severe", "severe", "moderate", "moderate", "moderate")


```



## Step 3 : Create seurat objects and remove ambient RNA 

```{r, load data}
setwd(datadir)

#This loop downloads all of the specified data files
#Converts them into Seaurt Objects, and stores them in a list 
Sce <- list()


#Converts them into Seaurt Objects, 
#Then runs decontX to remove ambient RNA
#And finally stores all of the objects as a list 
#and stores them in a list 
for (i in range) {
  #Create seurat objects and add mt% column 
  counts <- Read10X(Files[[i]])
  nam <- paste0(sample_label, i)
  seurat_object <- CreateSeuratObject(counts = counts, project = nam)
  seurat_object[["percent.mt"]] <- PercentageFeatureSet(seurat_object, pattern = "^MT-")
  #Then run decontX, using cellrnager raw counts matrix to calibrate 
  #counts.raw <- Read10X(Raws[[i]])
  #counts2 <- GetAssayData(object = seurat_object, slot = "counts")
  #sce_counts2 <- SingleCellExperiment(list(counts = counts2))
  #sce.raw <- SingleCellExperiment(list(counts = counts.raw))
  #sce_X <- decontX(sce_counts2, background = sce.raw)
  #seurat_object[["decontXcounts"]] <- CreateAssayObject(counts = decontXcounts(sce_X))
  #Then reset the default assay to "RNA" to run doublet finder
  DefaultAssay(seurat_object) <- "RNA"
  Sce[[i]] <- seurat_object
  #assign(nam, Sce[[i]])
}


```


## Step 4 : QC Threshold Filtering (For DoubletFinder) 
```{r, filter}
#This step removes low quality cells that do not meet the
#thresholding criteria specified in step 1 

#If repeating this step ourside of the automated pipeline, 
#you can set filter_round to whatever round of filtering this is 
filter_round <- c()

#If manually re-running filtering and setting new threshold values, specify here 
#mt_cutoff <- 30
#nfeature_cutoff <- 100
#ncount_cutoff <- 0

#these objects will store the counts after each round of filtering 
unfiltered_counts <- list()
filtered_sizes <- list()

for (i in range){
  #Store the raw cell counts
  filtered_sizes <- c(filtered_sizes, paste0(sample_label, i,"_raw_cells",filter_round))
  filtered_sizes <- c(filtered_sizes, ncol(Sce[[i]]))
  unfiltered_counts <- c(unfiltered_counts, ncol(Sce[[i]]))
  #Filter for mt% based on specified cutoff 
  Sce[[i]] <- subset(Sce[[i]], subset = percent.mt < mt_cutoff)
  #Save new cell counts after mt filtering
  filtered_sizes <- c(filtered_sizes, paste0(sample_label, i,"_mt_filtered",filter_round))
  filtered_sizes <- c(filtered_sizes, ncol(Sce[[i]]))
  #Filter for nFeature based on specified cutoff
  Sce[[i]] <- subset(Sce[[i]], subset = nFeature_RNA > nfeature_cutoff)
  #Save new cell counts after mt filtering
  filtered_sizes <- c(filtered_sizes, paste0(sample_label, i,"_nFeature_filtered",filter_round))
  filtered_sizes <- c(filtered_sizes, ncol(Sce[[i]]))
  #Filter for nCount based on specified cutoff
  Sce[[i]] <- subset(Sce[[i]], subset = nCount_RNA > ncount_cutoff)
  #Save new cell counts after mt filtering
  filtered_sizes <- c(filtered_sizes, paste0(sample_label, i,"_nCount_filtered",filter_round))
  filtered_sizes <- c(filtered_sizes, ncol(Sce[[i]]))
}

#Save the filtered cell counts
saveRDS(filtered_sizes, file = paste0(outdir,sample_label,"_filtered",filter_round,"_dims.rds"))


```

## Step 5 : Prepare Samples For DoubletFinder 
```{r}
#DoubletFinder requires that the RAW count matrix is normalized/scaled
#This is performed with SCTransform
#And that PCA and UMAP have been performed 
for (i in range){
  #In case this is being re-run, Setting default assay back to RNA
  DefaultAssay(Sce[[i]]) <- "RNA"
  #Run SCTransform  
  #Sce[[i]] <- SCTransform(Sce[[i]], verbose = FALSE)
  Sce[[i]] <- NormalizeData(Sce[[i]])
  Sce[[i]] <- FindVariableFeatures(Sce[[i]], selection.method = "vst", nfeatures = 2000)
  Sce[[i]] <- ScaleData(Sce[[i]])
  #Run PCA
  Sce[[i]] <- RunPCA(Sce[[i]])
  #Run UMAP
  Sce[[i]] <- RunUMAP(Sce[[i]], dims = 1:30)
}

```


## Step 6 : Run DoubletFinder 
```{r, DoubletFinder}
#The first step of DoubletFinder is calculating expected doublets for each sample 
#This loop determines this for each sample based on 
#raw returned cells saved before filtering 

#The expected number of doublets is assigned based on 10x Genomics' guidelines 
#https://kb.10xgenomics.com/hc/en-us/articles/360001378811-What-is-the-maximum-number-of-cells-that-can-be-profiled-

exp_d <- list()
for (i in range){
  #Total cells recovered 
  cells <- unfiltered_counts[[i]]
  #10x calcualtes expected doublets as a function of number of recovered cells 
  #Their table is for brackets of 1000 cells, so I am setting the boundaries at 500 cells
  #So that the bracket used is whatever 1000s is closest 
  bracket <- as.integer(cells/1000)
  bracket_up <- cells%%1000
  #Then this is a conditional to assign expected doublet rate   
  #Based on the bracket information 
  if (cells < 500) {
    exp_d[[i]] <- 0.004
    } else {
      exp_d[[i]] <- 0.008 * bracket
      if (bracket_up > 500) {
        exp_d[[i]] <- 0.008 * (bracket + 1)
      }
    }
}

#Then Run DoubletFinder, which will label expected doublets  

pK <- list()
nExp_poi <- list()
nExp_poi.adj <- list()
df_col <- list()
for (i in range ){
  #This simulates various pK values for this data to identify the optimal value 
  #I am specifying that 30 PCs were used in the original PCA (I could change 
  #This for the DF analysis, but choosing to use all PCs)
  #And specifying that I used SCTransform 
  sweep.res.list_tf1 <- paramSweep_v3(Sce[[i]], PCs = 1:30, sct = FALSE)
  #Generates a summary table of the above simultions. 
  #No multiplexed data so GT is false 
  sweep.stats_tf1 <- summarizeSweep(sweep.res.list_tf1, GT = FALSE)
  #Then this calculates BC metrics as a score of success for each pK value 
  bcmvn_tf1 <- find.pK(sweep.stats_tf1)
  #This goes through and pulls the optimal pK value after the simulations 
  pK_temp <- bcmvn_tf1 %>%
    filter(BCmetric == max(BCmetric)) %>%
    select(pK)
  pK[[i]] <- as.numeric(as.character(pK_temp[[1]]))
  #Adding the annotations of cluster results from the PCA 
  annotations <- Sce[[i]]@meta.data$ClusteringResults
  #Modelling homotypic doublets based on clustering patterns 
  homotypic.prop <- modelHomotypic(annotations)
  #Predicting the number of expected doublets based on exp_d from above 
  nExp_poi[[i]] <- round(exp_d[[i]]*nrow(Sce[[i]]@meta.data))  
  #Adjusting the total expected doublet count
  nExp_poi.adj[[i]] <- round(nExp_poi[[i]]*(1-homotypic.prop))
  #Running DoubletFinder 
  #I am setting PCs to 30 again 
  #The developers found that results are not significantly impacted by pN 
  #And so I followed their default value of 0.25 
  #pK and nExp are calculated above 
  #reuse.pANN is not being used, and SCTransform was used
  Sce[[i]] <- doubletFinder_v3(Sce[[i]], PCs = 1:30, pN = 0.25, pK = pK[[i]], nExp = nExp_poi[[i]], reuse.pANN = FALSE, sct = FALSE)
  #Sce[[i]] <- doubletFinder_v3(Sce[[i]], PCs = 1:30, pN = 0.25, pK = pK[[i]], nExp = nExp_poi.adj[[i]], reuse.pANN = paste0("pANN_0.25_",pK[[i]],"_",nExp_poi[[i]]), sct = FALSE)
  df_col[[i]] <- paste0("DF.classifications_0.25_",pK[[i]],"_",nExp_poi[[i]])
  #DimPlot(Sce[[i]], reduction = 'umap', group.by = df_col[[i]])
}

df_results <- list()
#Rename the doubletfinder results column so it is consistent for merging 
for (i in range){
  #pull the name of the df results meta data column 
  df_label <- df_col[[i]]
  #Paste a string to call the metadata column
  meta_col <- paste0("Sce[[i]]@meta.data$",df_label)
  #And paste a command to remove this column for later 
  remove_col <- paste0(meta_col," <- NULL")
  #Generate a temporary list of the doublet identities 
  col <- eval(parse(text=meta_col))
  df_results[[i]] <- col
  #Assign these doublet identities to the doublet column 
  #Sce[[i]]$doublet <- col
  #Then at the end, remove the DF column to clean up metadata
  #eval(parse(text=remove_col))
}

#Sce[[i]]$doublet
#df_results <- Sce[[i]]$doublet

#Sce[[i]]$doublet <- df_results
df_results_location <- paste0(datadir,sample_author,"/",sample_label,"_df_results.rds")
#saveRDS(df_results, file = df_results_location)
#saveRDS(df_results, file = paste0(datadir,sample_name,"/tamburini/thc_df_results.rds"))

```


## Step 7 : Reset Seurat Obejects, Run DecontX 

```{r, load data}
setwd(datadir)

#This loop downloads all of the specified data files
#Converts them into Seaurt Objects, and stores them in a list 
Sce <- list()


#Converts them into Seaurt Objects, 
#Then runs decontX to remove ambient RNA
#And finally stores all of the objects as a list 
#and stores them in a list 
for (i in range) {
  #Create seurat objects and add mt% column 
  counts <- Read10X(Files[[i]])
  nam <- paste0(sample_label, i)
  seurat_object <- CreateSeuratObject(counts = counts, project = nam)
  seurat_object[["percent.mt"]] <- PercentageFeatureSet(seurat_object, pattern = "^MT-")
  #Then run decontX, using cellrnager raw counts matrix to calibrate 
  counts.raw <- Read10X(Raws[[i]])
  counts2 <- GetAssayData(object = seurat_object, slot = "counts")
  sce_counts2 <- SingleCellExperiment(list(counts = counts2))
  sce.raw <- SingleCellExperiment(list(counts = counts.raw))
  sce_X <- decontX(sce_counts2, background = sce.raw)
  seurat_object[["decontXcounts"]] <- CreateAssayObject(counts = decontXcounts(sce_X))
  #Then set the default assay to "decontXcounts"
  DefaultAssay(seurat_object) <- "decontXcounts"
  Sce[[i]] <- seurat_object
  #assign(nam, Sce[[i]])
}


```


## Step 8 : Add Meta Data 
```{r add meta data, eval=FALSE}
for (i in range){
  Sce[[i]]$patient <- patient[[i]]
  Sce[[i]]$sex <- sex[[i]]
  Sce[[i]]$age <- age[[i]]
  Sce[[i]]$aetiology <- aetiology[[i]]
  Sce[[i]]$fibrosis_score <- fibrosis_score[[i]]
  Sce[[i]]$fibrosis_stage <- fibrosis_stage[[i]]
}
```


## Step 9 : QC Threshold Filtering (For Final Object Now) 
```{r, filter}
#This step removes low quality cells that do not meet the
#thresholding criteria specified in step 1 

#If repeating this step ourside of the automated pipeline, 
#you can set filter_round to whatever round of filtering this is 
filter_round <- c()

#If manually re-running filtering and setting new threshold values, specify here 
#mt_cutoff <- 30
#nfeature_cutoff <- 100
#ncount_cutoff <- 0

#these objects will store the counts after each round of filtering 
unfiltered_counts <- list()
filtered_sizes <- list()

for (i in range){
  #Store the raw cell counts
  filtered_sizes <- c(filtered_sizes, paste0(sample_label, i,"_raw_cells",filter_round))
  filtered_sizes <- c(filtered_sizes, ncol(Sce[[i]]))
  unfiltered_counts <- c(unfiltered_counts, ncol(Sce[[i]]))
  #Filter for mt% based on specified cutoff 
  Sce[[i]] <- subset(Sce[[i]], subset = percent.mt < mt_cutoff)
  #Save new cell counts after mt filtering
  filtered_sizes <- c(filtered_sizes, paste0(sample_label, i,"_mt_filtered",filter_round))
  filtered_sizes <- c(filtered_sizes, ncol(Sce[[i]]))
  #Filter for nFeature based on specified cutoff
  Sce[[i]] <- subset(Sce[[i]], subset = nFeature_RNA > nfeature_cutoff)
  #Save new cell counts after mt filtering
  filtered_sizes <- c(filtered_sizes, paste0(sample_label, i,"_nFeature_filtered",filter_round))
  filtered_sizes <- c(filtered_sizes, ncol(Sce[[i]]))
  #Filter for nCount based on specified cutoff
  Sce[[i]] <- subset(Sce[[i]], subset = nCount_RNA > ncount_cutoff)
  #Save new cell counts after mt filtering
  filtered_sizes <- c(filtered_sizes, paste0(sample_label, i,"_nCount_filtered",filter_round))
  filtered_sizes <- c(filtered_sizes, ncol(Sce[[i]]))
}

#Save the filtered cell counts
#saveRDS(filtered_sizes, file = paste0(outdir,sample_label,"_filtered",filter_round,"_dims.rds"))


```


#Step 10 : Merge the samples for one experiment  
```{r}
#Quickly make a list with shifted indexing for the merge function
sce_list_t <- list()
for (i in range){
  if (i != 1){
    m = i - 1
    sce_list_t[[m]] <- Sce[[i]]
  }
}

#And make a list of cell.ids to split the individual samples after this step
cell.ids <- paste0(sample_label,range)

#Create one large merged seurat object
object_merged <- merge(Sce[[1]], y = sce_list_t, add.cell.ids = cell.ids, project = paste0(sample_label,"_merged"))

#Add a column in the meta data to specify where the sample came from 
experiment_group <- list()
for (i in ncol(object_merged)){
  experiment_group <- c(experiment_group, sample_label)
}
#@meta.data
object_merged$exp_group <- experiment_group

#rename the merged object based on the sample label
merged_name <- paste0(sample_label,"_merged")
assign(merged_name, object_merged)

####

```

## Save Outputs
```{r Save}
saveRDS(df_results, file = df_results_location)
saveRDS(filtered_sizes, file = paste0(outdir,sample_label,"_filtered",filter_round,"_dims.rds"))
saveRDS(object_merged, file = paste0(outdir,sample_label,"_merged_object.rds"))
saveRDS(pK, file = paste0(outdir,sample_label,"_pK.rds"))
```


