# DoubletFinder Eddie

library(dplyr)
library(Seurat)
library(patchwork)
library(DoubletFinder)
#library(tidyverse)
library(ggplot2)

range <- 1:2
sample_author <- "tamburini"
sample_name <- "tamburini_healthy_"
sample_label <- "thc"

datadir <- "/exports/eddie/scratch/s2249132/conda_test/"
#datadir <- "/Volumes/cmvm/scs/groups/pramacha-GROUP/Max_Hammer/SeuratObjects/"
outdir <- "/exports/eddie/scratch/s2249132/conda_test/"

counts <- readRDS(paste0(datadir,sample_author,"/",sample_label,"_filtered_dims.rds"))


Files <- list()
Destination <- list()
for (i in range){
  Files[[i]] <- paste0(datadir,sample_author,"/",sample_label, i,"_dx_SeuratObject.rds")
  Destination[[i]] <- paste0(outdir,sample_name,i,"_DF.rds")
}

Sce <- list()

for (i in range){
  Sce[[i]] <- readRDS(Files[[i]])
  #In case this is being re-run, Setting default assay back to RNA
  DefaultAssay(Sce[[i]]) <- "RNA"
  #Run SCTransform  
  #Sce[[i]] <- SCTransform(Sce[[i]], verbose = FALSE)
  Sce[[i]] <- NormalizeData(Sce[[i]])
  Sce[[i]] <- FindVariableFeatures(Sce[[i]], selection.method = "vst", nfeatures = 2000)
  Sce[[i]] <- ScaleData(Sce[[i]])
  #Run PCA
  Sce[[i]] <- RunPCA(Sce[[i]])
  #Run UMAP
  Sce[[i]] <- RunUMAP(Sce[[i]], dims = 1:30)
}


# Run DF
exp_d <- list()
for (i in range){
  #Total cells recovered 
  cells <- counts[[(i*8 - 6)]]
  #cells <- 130
  #10x calcualtes expected doublets as a function of number of recovered cells 
  #Their table is for brackets of 1000 cells, so I am setting the boundaries at 500 cells
  #So that the bracket used is whatever 1000s is closest 
  bracket <- as.integer(cells/1000)
  bracket_up <- cells%%1000
  #Then this is a conditional to assign expected doublet rate   
  #Based on the bracket information 
  if (cells < 500) {
    exp_d[[i]] <- 0.004
  } else {
    exp_d[[i]] <- 0.008 * bracket
    if (bracket_up > 500) {
      exp_d[[i]] <- 0.008 * (bracket + 1)
    }
  }
}

#Then Run DoubletFinder, which will label expected doublets  

pK <- list()
nExp_poi <- list()
nExp_poi.adj <- list()
df_col <- list()
for (i in range ){
  #This simulates various pK values for this data to identify the optimal value 
  #I am specifying that 30 PCs were used in the original PCA (I could change 
  #This for the DF analysis, but choosing to use all PCs)
  #And specifying that I used SCTransform 
  sweep.res.list_tf1 <- paramSweep_v3(Sce[[i]], PCs = 1:30, sct = FALSE)
  #Generates a summary table of the above simultions. 
  #No multiplexed data so GT is false 
  sweep.stats_tf1 <- summarizeSweep(sweep.res.list_tf1, GT = FALSE)
  #Then this calculates BC metrics as a score of success for each pK value 
  bcvals <- c()
  bcstats <- c()
  for (i in 1:23){
    bcvals <- c()
    for (n in 1:6){
      position <- (i + 23*(n -1))
      bcvals <- c(bcvals, sweep.stats_tf1[position, 3])
    }
    bcmean <- mean(bcvals)
    bcvar <- var(bcvals)
    bcstat <- bcmean/bcvar
    if (bcstat > max(bcstats)){
      max_pK_pos <- i
    }
    bcstats <- c(bcstats, bcstat)
  }
  #This goes through and pulls the optimal pK value after the simulations 
  pK_temp <- sweep.stats_tf1[max_pK_pos, 2]
  pK[[i]] <- as.numeric(as.character(pK_temp[[1]]))
  #Adding the annotations of cluster results from the PCA 
  annotations <- Sce[[i]]@meta.data$ClusteringResults
  #Modelling homotypic doublets based on clustering patterns 
  homotypic.prop <- modelHomotypic(annotations)
  #Predicting the number of expected doublets based on exp_d from above 
  nExp_poi[[i]] <- round(exp_d[[i]]*nrow(Sce[[i]]@meta.data))  
  #Adjusting the total expected doublet count
  nExp_poi.adj[[i]] <- round(nExp_poi[[i]]*(1-homotypic.prop))
  #Running DoubletFinder 
  #I am setting PCs to 30 again 
  #The developers found that results are not significantly impacted by pN 
  #And so I followed their default value of 0.25 
  #pK and nExp are calculated above 
  #reuse.pANN is not being used, and SCTransform was used
  Sce[[i]] <- doubletFinder_v3(Sce[[i]], PCs = 1:30, pN = 0.25, pK = pK[[i]], nExp = nExp_poi[[i]], reuse.pANN = FALSE, sct = FALSE)
  #Sce[[i]] <- doubletFinder_v3(Sce[[i]], PCs = 1:30, pN = 0.25, pK = pK[[i]], nExp = nExp_poi.adj[[i]], reuse.pANN = paste0("pANN_0.25_",pK[[i]],"_",nExp_poi[[i]]), sct = FALSE)
  df_col[[i]] <- paste0("DF.classifications_0.25_",pK[[i]],"_",nExp_poi[[i]])
  #DimPlot(Sce[[i]], reduction = 'umap', group.by = df_col[[i]])
}

df_results <- list()
#Rename the doubletfinder results column so it is consistent for merging 
for (i in range){
  #pull the name of the df results meta data column 
  df_label <- df_col[[i]]
  #Paste a string to call the metadata column
  meta_col <- paste0("Sce[[i]]@meta.data$",df_label)
  #And paste a command to remove this column for later 
  remove_col <- paste0(meta_col," <- NULL")
  #Generate a temporary list of the doublet identities 
  col <- eval(parse(text=meta_col))
  df_results[[i]] <- col
  #Assign these doublet identities to the doublet column 
  Sce[[i]]$doublet <- col
  #Then at the end, remove the DF column to clean up metadata
  #eval(parse(text=remove_col))
}

#Then load in original object, add DF column,  and save 
for (i in range){
  Sce2[[i]] <- readRDS(Files[[i]])
  Sce2[[i]]$DoubletFinder <- Sce[[i]]$doublet
  saveRDS(Sce2[[i]], file = paste0(outdir,fig_lab,i,"_DF_object.rds"))
}

